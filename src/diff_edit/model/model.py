import cv2
import torch
import logging

import numpy as np

from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler, StableDiffusionInpaintPipeline
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm

# from fastai.basics import show_images


class DiffEdit():
    def __init__(self):
        # attributes for the components
        self.tokenizer: CLIPTokenizer = None
        self.text_encoder: CLIPTextModel = None

        self.vae: AutoencoderKL = None
        self.unet: UNet2DConditionModel = None
        self.inpainting: StableDiffusionInpaintPipeline = None

        self.scheduler: LMSDiscreteScheduler = None

        # attributes for the device
        self.torch_device: str = None

        # attributes for the VAE
        self.vae_const = 0.18215

        # attributes for the DiffEdit model
        self.is_ready = False

        # seed for reproducibility
        self.seed = torch.seed()

        # attributes for the mask
        self._rough_mask = None
        self._processed_mask = None

    def get_mask(self, im_path, p1, p2, seed=torch.seed(), n=10):
        """
            This method returns the mask generated by the DiffEdit algorithm.

            :param im_path: The path to the image to edit.
            :param p1: The prompt to remove.
            :param p2: The prompt to add.
            :param seed: The seed to use for reproducibility.
            :param n: The number of iterations to perform to get the mask. Each iteration is a diffusion process.

            :return: A list containing the processed mask, the rough mask, the blended mask (visualization only)
        """
        im_path = Path(im_path)
        im = Image.open(im_path).resize((512, 512))
        im_latent = self._img2latent(im)
        mask = self._calc_diffedit_mask(im_latent, p1, p2, n, seed)
        return self._processed_mask, self._rough_mask, self._get_blended_mask(im, mask)

    def refine_mask(self, mask, n=10):
        """
            This method refines the mask generated by the DiffEdit algorithm.
            E.G. Mix togheter mask coming from different prompts pairs.
        """
        pass

    def inpaint_mask(self, img, mask, prompt, seed=torch.seed()):
        """
            This method performs the inpainting of the image using the provided mask.
            :param img: The image to inpaint.
            :param mask: The mask to use for inpainting.
            :param prompt: The prompt to use for inpainting.
            :param seed: The seed to use for reproducibility.

            :return: The inpainted image.
        """
        return self.inpainting(prompt=[prompt], image=img, mask_image=mask,
                               generator=torch.Generator(self.torch_device).manual_seed(seed)).images[0]

    def demo_diffedit(self, im_path, p1, p2, seed=torch.seed(), n=10):
        """
        This method performs the DiffEdit algorithm on the specified image.
        :param im_path: The path to the image to edit.
        :param p1: The prompt to remove.
        :param p2: The prompt to add.
        :param seed: The seed to use for reproducibility.
        :param n: The number of iterations to perform to get the mask. Each iteration is a diffusion process.

        :return: A list containing the original image, the original image with mask and the resulting inpainted image.
        """
        im_path = Path(im_path)
        out = []

        im = Image.open(im_path).resize((512, 512))
        im_latent = self._img2latent(im)
        out.append(im)

        logging.info(f"Obtaining the mask by running the diffusion process {n} times.")
        # mask = self._calc_diffedit_mask(im_latent, p1, p2, n)
        # out.append(self._get_blended_mask(im, mask))
        mask, rough_mask, blended_mask = self.get_mask(im_path, p1, p2, seed, n)
        out.append(blended_mask)

        logging.info(f"Inpainting the image using the mask.")
        # out.append(self.inpainting(prompt=[p2], image=im, mask_image=mask,
        #                            generator=torch.Generator(self.torch_device).manual_seed(seed)).images[0])
        out.append(self.inpaint_mask(im, mask, p2, seed))
        return out

    def to(self, device):
        """
            This method moves the components of the model to the specified device.
            :param device: The device to move the components to. Valid values are "cpu", "mps" and "cuda".
            :return: self
        """
        self._assert_ready()

        # make sure the device is valid and available
        if device == "mps":
            if not torch.backends.mps.is_built():
                raise Exception("MPS is not available, please use cpu or cuda instead.")
        elif device == "cuda":
            if not torch.cuda.is_available():
                raise Exception("CUDA is not available, please use cpu or mps instead.")
        elif device != "cpu":
            raise Exception("Invalid device, please use cpu, mps or cuda. Received: " + str(device) + " instead.")

        self.torch_device = device

        # move the components to the device
        self.vae = self.vae.to(device)
        self.text_encoder = self.text_encoder.to(device)
        self.unet = self.unet.to(device)
        self.inpainting = self.inpainting.to(device)
        return self



    def compose_model(self, vae_model, tokenizer, text_encoder, unet, inpainting, scheduler, torch_dev="cpu"):
        """
            This method composes the model from the specified components.
            :param vae_model: The name of the VAE model to use. E.g. "stabilityai/sd-vae-ft-ema".
            :param tokenizer: The name of the tokenizer to use. E.g. "openai/clip-vit-large-patch14".
            :param text_encoder: The name of the text encoder to use. E.g. "openai/clip-vit-large-patch14".
            :param unet: The name of the UNet model to use. E.g. "CompVis/stable-diffusion-v1-4".
            :param scheduler: The name of the scheduler to use. E.g. "LMSDiscreteScheduler".
            :param torch_dev: The device to use. Valid values are "cpu", "mps" and "cuda".
            :return: self
        """
        # load the components
        tokenizer = CLIPTokenizer.from_pretrained(tokenizer)
        text_encoder = CLIPTextModel.from_pretrained(text_encoder)

        vae = AutoencoderKL.from_pretrained(vae_model)
        unet = UNet2DConditionModel.from_pretrained(unet, subfolder="unet")

        # load RunwayML's Inpainting Model
        inpainting = StableDiffusionInpaintPipeline.from_pretrained(inpainting)

        # attributes for the components
        self.tokenizer = tokenizer
        self.text_encoder = text_encoder
        self.vae = vae
        self.unet = unet
        self.inpainting = inpainting
        self.scheduler = scheduler

        # al DiffEdit components are ready
        self.is_ready = True

        # move the components to the chosen device
        self.to(torch_dev)

    def _check_components(self):
        logging.debug("self.tokenizer is None: ", self.tokenizer is None, "\n",
                      "self.text_encoder is None: ", self.text_encoder is None, "\n",
                      "self.unet is None: ", self.unet is None, "\n",
                      "self.inpainting is None: ", self.inpainting is None, "\n",
                      "self.scheduler is None: ", self.scheduler is None, "\n",
                      "self.torch_device is None: ", self.torch_device is None, "\n")

    def _assert_ready(self):
        self._check_components()
        # make sure the components are initialized
        assert self.is_ready, "DiffEdit model is not initialized, compose_model() must be called first."

    def _img2latent(self, im):
        self._assert_ready()

        # convert image to tensor
        im = transforms.ToTensor()(im).unsqueeze(0)
        with torch.no_grad():
            # encode the image into latent space through the VAE
            latent = self.vae.encode(im.to(self.torch_device) * 2 - 1)
        latent = latent.latent_dist.sample() * self.vae_const
        return latent

    def _latents2imgs(self, latents):
        self._assert_ready()

        # getting the latents from VAE (decoder layers)
        latents = latents * 1 / self.vae_const
        with torch.no_grad():
            imgs = self.vae.decode(latents).sample
        # let's convert images to PIL so we can display them.
        imgs = (imgs / 2 + 0.5).clamp(0, 1)
        imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()
        imgs = (imgs * 255).round().astype("uint8")
        imgs = [Image.fromarray(im) for im in imgs]
        return imgs

    def _get_embedding_for_prompt(self, prompt, **kwargs):
        self._assert_ready()

        max_length = self.tokenizer.model_max_length
        tokens = self.tokenizer([prompt], padding="max_length", max_length=max_length, truncation=True,
                                return_tensors="pt")
        with torch.no_grad():  # we are using for inference, no gradients needed
            return self.text_encoder(tokens.input_ids.to(self.torch_device))[0]

    # Given a starting image latent and a prompt; predict the noise that should be removed to transform
    # the noised source image to a denoised image guided by the prompt.
    def _predict_noise(self, text_embeddings, im_latents, seed=torch.seed(), guidance_scale=7, strength=0.5,
                       num_inference_steps=50):
        self._assert_ready()

        # num_inference_steps is Number of denoising steps
        torch.manual_seed(seed)  # initial latent noise

        uncond = self._get_embedding_for_prompt('')  # unconditional prompt
        text_embeddings = torch.cat([uncond, text_embeddings])  # concatenate unconditional and conditional prompts

        # Prep Scheduler
        self.scheduler.set_timesteps(num_inference_steps)

        offset = self.scheduler.config.get("steps_offset", 0)
        init_timestep = int(num_inference_steps * strength) + offset
        init_timestep = min(init_timestep, num_inference_steps)

        timesteps = self.scheduler.timesteps[-init_timestep]
        timesteps = torch.tensor([timesteps] * 1, device=self.torch_device).float()  # [timesteps] * 1 * 1

        noise = torch.randn_like(im_latents)
        latents = self.scheduler.add_noise(im_latents, noise, timesteps=timesteps)
        latents = latents.to(self.torch_device).float()

        t_start = max(num_inference_steps - init_timestep + offset, 0)
        timesteps = self.scheduler.timesteps[t_start:].to(self.torch_device)

        noisy_latent = latents.clone()

        noise_pred = None
        for i, tm in enumerate(timesteps):
            latent_model_input = torch.cat([latents] * 2)
            latent_model_input = self.scheduler.scale_model_input(latent_model_input, tm)

            # predict the noise residual
            with torch.no_grad():
                noise_pred = self.unet(latent_model_input, tm, encoder_hidden_states=text_embeddings)["sample"]

            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)

            u = noise_pred_uncond
            g = guidance_scale
            t = noise_pred_text

            # perform guidance
            noise_pred = u + g * (t - u)

            # compute the previous noisy sample x_t -> x_t-1
            latents = self.scheduler.step(noise_pred, tm, latents).prev_sample

        return self._latents2imgs(latents)[0], noise_pred

    # For a given reference prompt and a query prompt;
    # Run the diffusion process 10 times; Calculating a "noise distance" for each sample
    def _calc_diffedit_samples(self, encoded, prompt1, prompt2, n=2, seed=torch.seed(), **kwargs):
        diffs = []
        logging.info(f"Running {n} times the diffusion process to calculate a 'noise distance' for each sample.")
        # So we can reproduce mask generation we generate a list of n seeds
        torch.manual_seed(seed)
        seeds = torch.randint(0, 2 ** 62, (n,)).tolist()
        for i in tqdm(range(n)):
            seed = seeds[i]  # Important to use same seed for the two noise samples
            emb1 = self._get_embedding_for_prompt(prompt1)
            _im1, n1 = self._predict_noise(emb1, encoded, seed)
            emb2 = self._get_embedding_for_prompt(prompt2)
            _im2, n2 = self._predict_noise(emb2, encoded, seed)

            # Aggregate the channel components by taking the Euclidean distance.
            diffs.append((n1 - n2)[0].pow(2).sum(dim=0).pow(0.5)[None])
        all_masks = torch.cat(diffs)
        return all_masks

    # Given an image latent and two prompts; generate a grayscale diff by sampling the noise predictions
    # between the prompts.
    def _calc_diffedit_diff(self, im_latent, p1, p2, n, seed=torch.seed()):
        m = self._calc_diffedit_samples(im_latent, p1, p2, n, seed)
        m = m.mean(axis=0)  # average samples together
        m = (m - m.min()) / (m.max() - m.min())  # rescale to interval [0,1]
        m = (m * 255.).cpu().numpy().astype(np.uint8)  # rescale to [0, 255]
        m = Image.fromarray(m)  # convert to PIL
        return m

    # Try to improve the mask through convolutions, this method assumes m is a PIL object containing a grayscale 'diff'
    def _process_diffedit_mask(self, mask, threshold=0.35):
        mask = np.array(np.array(mask).astype(np.float32))  # convert to numpy
        mask = cv2.GaussianBlur(mask, (5, 5), 1)
        mask = (mask > (255. * threshold)).astype(np.float32) * 255  # threshold and convert back to [0,255]
        mask = Image.fromarray(mask.astype(np.uint8))  # convert to PIL
        return mask

    # Given an image latent and two prompts; generate a binarized mask (PIL) appropriate for inpainting
    def _calc_diffedit_mask(self, im_latent, p1, p2, n=10, seed=torch.seed()):
        m = self._calc_diffedit_diff(im_latent, p1, p2, n, seed)
        self._rough_mask = m.copy()
        m = self._process_diffedit_mask(m)
        self._processed_mask = m.copy()
        m = m.resize((512, 512))
        return m

    # Composite the mask over the provided image; for demonstration purposes
    def _get_blended_mask(self, im, mask_gray):  # Both expected to be PIL images
        mask_rgb = mask_gray.convert('RGB')
        return Image.blend(im, mask_rgb, 0.40)

    # Show the original image, the original image with mask and the resulting inpainted image
